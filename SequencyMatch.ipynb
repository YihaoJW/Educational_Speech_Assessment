{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import glob\n",
    "import plistlib\n",
    "from typing import Union, List, Dict, Tuple, Iterable\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jackwang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jackwang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download punkt and pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# A function that read a path of *.plist file and return a list of dictionaries and the name of the file without the extension\n",
    "def read_plist(path: str) -> Tuple[List[Dict], str]:\n",
    "    with open(path, 'rb') as f:\n",
    "        data = plistlib.load(f)\n",
    "    return data, path.split('/')[-1].split('.')[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Read all *.plist files under Result and create a generator that iter over all files\n",
    "file_list = glob.glob('Result/*.plist')\n",
    "file_generator = map(read_plist, file_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%#read the file\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Generate a sample\n",
    "it = iter(file_generator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# create a dict with key as passage id and value as passage using information in the csv files\n",
    "def create_passage_dict(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    passage_dict = {row['passage_id'] : row['passage'] for _, row in df.iterrows()}\n",
    "    return passage_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "passage_dict = create_passage_dict(\"yrs12_passages.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# create a function that parse a string to a dict with sample\n",
    "# the string has schema student_{student_id}_passage_{passage_id}_{random_number}\n",
    "def parse_files_name(string):\n",
    "    student_id = string.split('_')[1]\n",
    "    passage_id = int(string.split('_')[3])\n",
    "    random_number = string.split('_')[4]\n",
    "    return {'student_id': student_id, 'passage_id': passage_id, 'random_number': random_number}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# a function read a string defined in schema student_{student_id}_passage_{passage_id}_{random_number} and return tokenized text remove stop words and punctuation\n",
    "def read_and_tokenize_file(file_name):\n",
    "    idx = parse_files_name(file_name)['passage_id']\n",
    "    textx = passage_dict[idx]\n",
    "    tokens = nltk.word_tokenize(textx)\n",
    "    return [{\"tString\" : token.lower(), \"tConfidence\": -1} for token in tokens if token.isalpha()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# A function that input an iterator return the first item that is no error\n",
    "# the iterator contains a tuple of (data, file_name)\n",
    "# map the file name with to a passage using read_and_tokenize_file\n",
    "# return a tuple of ((tokenized_text, data), file_name)\n",
    "# if there is a KeyError, it will return the next item\n",
    "def get_next_item(it):\n",
    "    while True:\n",
    "        try:\n",
    "            data, file_name = next(it)\n",
    "            tokenized_text = read_and_tokenize_file(file_name)\n",
    "            return (tokenized_text, data), file_name\n",
    "        except KeyError:\n",
    "            continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#A function take two lists of words in sequence as input and return using Longest Common Subsequence algorithm\n",
    "def lcss(ref_text:List[Dict], student_input:List[Dict]) -> List[Dict]:\n",
    "    lengths = [[0 for j in range(len(student_input) + 1)] for i in range(len(ref_text) + 1)]\n",
    "    # row 0 and column 0 are initialized to 0 already\n",
    "    for i, rec_x in enumerate(ref_text):\n",
    "        x = rec_x['tString']\n",
    "        for j, rec_y in enumerate(student_input):\n",
    "            y = rec_y['tString'].lower()\n",
    "            if x == y and rec_y['tConfidence'] >= 0.01:\n",
    "                lengths[i+1][j+1] = lengths[i][j] + 1\n",
    "            else:\n",
    "                lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    "    # read the substring out from the matrix\n",
    "    result = []\n",
    "    x, y = len(ref_text), len(student_input)\n",
    "    while x != 0 and y != 0:\n",
    "        if lengths[x][y] == lengths[x-1][y]:\n",
    "            result.append((ref_text[x - 1], \"R\"))\n",
    "            x -= 1\n",
    "        elif lengths[x][y] == lengths[x][y-1]:\n",
    "            result.append((student_input[y - 1], \"A\"))\n",
    "            y -= 1\n",
    "        else:\n",
    "            assert ref_text[x - 1]['tString'] == student_input[y - 1]['tString'].lower()\n",
    "            result.append((student_input[y - 1], \"M\"))\n",
    "            x -= 1\n",
    "            y -= 1\n",
    "    return result[::-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "common = lcss(*get_next_item(it)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# A function that input a list of tuple of (word, confidence, status) and return a string of word with status\n",
    "# return confidence with 2 digits if the status is M otherwise add the status as a tag at the end of the word\n",
    "def get_string_with_status(lcs_matching_result):\n",
    "    common_tuple = [(x['tString'], x['tConfidence'], c) for x, c in lcs_matching_result]\n",
    "    return ' '.join([f'{word}<{status}>' if status != 'M' else f'{word}<{confidence:.2f}>' for word, confidence, status in common_tuple])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "'monte<R> loved<0.17> and<A> cooking<0.59> for<0.67> people<0.69> his<0.65> favorite<0.64> thing<0.69> to<R> make<R> was<0.83> tacos<0.86> and<0.86> refried<0.85> beans<0.89> then<A> when<R> he<0.86> made<0.85> them<0.87> and<A> he<0.86> felt<0.85> like<0.81> a<0.46> master<0.80> chief<A> shift<A> chef<R>'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_string_with_status(common)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[({'tString': 'monte', 'tConfidence': -1}, 'R'),\n ({'tConfidence': 0.16599999368190765,\n   'tDuration': 0.48,\n   'tString': 'loved',\n   'tTime': 3.33},\n  'M'),\n ({'tConfidence': 0.014999999664723873,\n   'tDuration': 0.1499999999999999,\n   'tString': 'and',\n   'tTime': 3.81},\n  'A'),\n ({'tConfidence': 0.5920000076293945,\n   'tDuration': 0.6500000000000004,\n   'tString': 'cooking',\n   'tTime': 3.96},\n  'M'),\n ({'tConfidence': 0.6660000085830688,\n   'tDuration': 0.25,\n   'tString': 'for',\n   'tTime': 4.61},\n  'M'),\n ({'tConfidence': 0.6919999718666077,\n   'tDuration': 0.5699999999999994,\n   'tString': 'people',\n   'tTime': 4.86},\n  'M'),\n ({'tConfidence': 0.652999997138977,\n   'tDuration': 0.33000000000000007,\n   'tString': 'his',\n   'tTime': 5.55},\n  'M'),\n ({'tConfidence': 0.6389999985694885,\n   'tDuration': 0.4500000000000002,\n   'tString': 'favorite',\n   'tTime': 5.88},\n  'M'),\n ({'tConfidence': 0.6890000104904175,\n   'tDuration': 0.2999999999999998,\n   'tString': 'thing',\n   'tTime': 6.33},\n  'M'),\n ({'tString': 'to', 'tConfidence': -1}, 'R'),\n ({'tString': 'make', 'tConfidence': -1}, 'R'),\n ({'tConfidence': 0.8330000042915344,\n   'tDuration': 0.4500000000000002,\n   'tString': 'was',\n   'tTime': 6.63},\n  'M'),\n ({'tConfidence': 0.8560000061988831,\n   'tDuration': 0.6899999999999995,\n   'tString': 'tacos',\n   'tTime': 7.23},\n  'M'),\n ({'tConfidence': 0.8569999933242798,\n   'tDuration': 0.33999999999999986,\n   'tString': 'and',\n   'tTime': 7.92},\n  'M'),\n ({'tConfidence': 0.8510000109672546,\n   'tDuration': 0.8899999999999988,\n   'tString': 'refried',\n   'tTime': 8.290000000000001},\n  'M'),\n ({'tConfidence': 0.8899999856948853,\n   'tDuration': 0.75,\n   'tString': 'beans',\n   'tTime': 9.18},\n  'M'),\n ({'tConfidence': 0.8759999871253967,\n   'tDuration': 0.34999999999999964,\n   'tString': 'then',\n   'tTime': 10.89},\n  'A'),\n ({'tString': 'when', 'tConfidence': -1}, 'R'),\n ({'tConfidence': 0.8629999756813049,\n   'tDuration': 0.16000000000000014,\n   'tString': 'he',\n   'tTime': 11.24},\n  'M'),\n ({'tConfidence': 0.8539999723434448,\n   'tDuration': 0.39000000000000057,\n   'tString': 'made',\n   'tTime': 11.4},\n  'M'),\n ({'tConfidence': 0.8709999918937683,\n   'tDuration': 0.33000000000000007,\n   'tString': 'them',\n   'tTime': 11.790000000000001},\n  'M'),\n ({'tConfidence': 0.8560000061988831,\n   'tDuration': 0.23999999999999844,\n   'tString': 'and',\n   'tTime': 12.120000000000001},\n  'A'),\n ({'tConfidence': 0.8560000061988831,\n   'tDuration': 0.21000000000000085,\n   'tString': 'he',\n   'tTime': 12.36},\n  'M'),\n ({'tConfidence': 0.8489999771118164,\n   'tDuration': 0.3699999999999992,\n   'tString': 'felt',\n   'tTime': 12.57},\n  'M'),\n ({'tConfidence': 0.8109999895095825,\n   'tDuration': 0.3500000000000014,\n   'tString': 'like',\n   'tTime': 12.94},\n  'M'),\n ({'tConfidence': 0.460999995470047,\n   'tDuration': 0.22999999999999865,\n   'tString': 'a',\n   'tTime': 13.290000000000001},\n  'M'),\n ({'tConfidence': 0.7990000247955322,\n   'tDuration': 0.5399999999999991,\n   'tString': 'master',\n   'tTime': 13.56},\n  'M'),\n ({'tConfidence': 0.8759999871253967,\n   'tDuration': 0.620000000000001,\n   'tString': 'chief',\n   'tTime': 14.1},\n  'A'),\n ({'tConfidence': 0.6650000214576721,\n   'tDuration': 0.7799999999999994,\n   'tString': 'shift',\n   'tTime': 15.540000000000001},\n  'A'),\n ({'tString': 'chef', 'tConfidence': -1}, 'R')]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# iterate over all files in file_list and save the result by file name under a new folder Match with extension .txt and content is get_string_with_status(common_tuple)\n",
    "# if there is a KeyError, it will skip the file\n",
    "# overwrite the file if it already exists\n",
    "\n",
    "def save_match_result(file_list):\n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            data, file_name = read_plist(file_path)\n",
    "            tokenized_text = read_and_tokenize_file(file_name)\n",
    "            common = lcss(tokenized_text, data)\n",
    "            with open(f'Match/{file_name}.txt', 'w') as f:\n",
    "                f.write(get_string_with_status(common))\n",
    "        except KeyError:\n",
    "            continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "save_match_result(file_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# iterate over all files in file_list and save the result by csv table with index as file name and content is get_string_with_status(common_tuple)\n",
    "# if there is a KeyError, it will skip the file\n",
    "# sort the table by file name\n",
    "# compress with gzip and overwrite the file if it already exists\n",
    "def save_result_csv(file_list):\n",
    "    result = []\n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            data, file_name = read_plist(file_path)\n",
    "            tokenized_text = read_and_tokenize_file(file_name)\n",
    "            common = lcss(tokenized_text, data)\n",
    "            result.append((file_name, get_string_with_status(common)))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    df = pd.DataFrame(result, columns=['file_name', 'result'])\n",
    "    df = df.sort_values(by=['file_name'])\n",
    "    df.to_csv('result.csv.gz', index=False, compression='gzip')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "save_result_csv(file_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
