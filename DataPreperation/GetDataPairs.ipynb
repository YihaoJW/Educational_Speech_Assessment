{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackwang/.local/share/virtualenvs/Code-_CZGGnvj/lib/python3.9/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/Users/jackwang/.local/share/virtualenvs/Code-_CZGGnvj/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: [\"[Errno 2] The file to load file system plugin from does not exist.: '/Users/jackwang/.local/share/virtualenvs/Code-_CZGGnvj/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so'\"]\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/Users/jackwang/.local/share/virtualenvs/Code-_CZGGnvj/lib/python3.9/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/Users/jackwang/.local/share/virtualenvs/Code-_CZGGnvj/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: [\"dlopen(/Users/jackwang/.local/share/virtualenvs/Code-_CZGGnvj/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io.so, 0x0006): tried: '/Users/jackwang/.local/share/virtualenvs/Code-_CZGGnvj/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/jackwang/.local/share/virtualenvs/Code-_CZGGnvj/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file), '/Users/jackwang/.local/share/virtualenvs/Code-_CZGGnvj/lib/python3.9/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file)\"]\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import plistlib\n",
    "from typing import Union, List, Dict, Tuple, Iterable\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "from scipy.io import wavfile\n",
    "import tensorflow_io as tfio\n",
    "from pydub import AudioSegment\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython import display\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# A function take two lists of words in sequence as input and return using The Longest Common Subsequence algorithm\n",
    "def lcss(ref_text: List[Dict], student_input: List[Dict], prob: float = 0.1, stemmer: nltk.stem.api = None) ->  Tuple[List[Tuple[Dict, str]],List[int],List[int]]:\n",
    "    \"\"\"\n",
    "    A function take two lists of words in sequence as input and return using The Longest Common Subsequence algorithm\n",
    "\n",
    "    :param ref_text: a list of words in sequence that each element is a dict with key must contain as tString and\n",
    "    tConfidence\n",
    "\n",
    "    :param student_input: a list of words in sequence that each element is a dict with key must contain\n",
    "    as tString and tConfidence\n",
    "\n",
    "    :param stemmer: a stemmer object from nltk.Stem\n",
    "    :param prob: a float number that is the threshold of confidence\n",
    "\n",
    "    :return: a list of tuple of (dict, status) where dict is a dict with key contain as\n",
    "    tString and tConfidence\n",
    "    \"\"\"\n",
    "    stemming = stemmer is not None\n",
    "    lengths = [[0 for j in range(len(student_input) + 1)] for i in range(len(ref_text) + 1)]\n",
    "    # row 0 and column 0 are initialized to 0 already\n",
    "    for i, rec_x in enumerate(ref_text):\n",
    "        x = (stemmer.stem(rec_x['tString']) if stemming else rec_x['tString']).lower()\n",
    "        for j, rec_y in enumerate(student_input):\n",
    "            y = (stemmer.stem(rec_y['tString']) if stemming else rec_y['tString']).lower()\n",
    "            if x == y and rec_y['tConfidence'] >= prob:\n",
    "                lengths[i + 1][j + 1] = lengths[i][j] + 1\n",
    "            else:\n",
    "                lengths[i + 1][j + 1] = max(lengths[i + 1][j], lengths[i][j + 1])\n",
    "    # read the substring out from the matrix\n",
    "    result = []\n",
    "    pos_x, pos_y = [], []\n",
    "    x, y = len(ref_text), len(student_input)\n",
    "    while x != 0 and y != 0:\n",
    "        if lengths[x][y] == lengths[x - 1][y]:\n",
    "            result.append((ref_text[x - 1], \"R\"))\n",
    "            x -= 1\n",
    "        elif lengths[x][y] == lengths[x][y - 1]:\n",
    "            result.append((student_input[y - 1], \"A\"))\n",
    "            y -= 1\n",
    "        else:\n",
    "            try:\n",
    "                if not stemming:\n",
    "                    assert ref_text[x - 1]['tString'].lower() == student_input[y - 1]['tString'].lower()\n",
    "                else:\n",
    "                    assert stemmer.stem(ref_text[x - 1]['tString'].lower()) == stemmer.stem(student_input[y - 1]['tString'].lower())\n",
    "            except AssertionError:\n",
    "                print(\"Error: \", ref_text[x - 1]['tString'].lower(), student_input[y - 1]['tString'].lower(), ref_text[x - 1]['tString'].lower()== student_input[y - 1]['tString'].lower() )\n",
    "                raise AssertionError\n",
    "            result.append((student_input[y - 1], \"M\"))\n",
    "            pos_x.append(x - 1)\n",
    "            pos_y.append(y - 1)\n",
    "            x -= 1\n",
    "            y -= 1\n",
    "    return result[::-1], pos_x[::-1], pos_y[::-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def read_plist(path: str) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Read a plist file and return a list of dictionaries and the name of the file without the extension\n",
    "\n",
    "    :param path:\n",
    "    Path str to the plist file\n",
    "\n",
    "    :return: a tuple of (list of dictionaries, file name) that dictionary is word information that student read\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        data = plistlib.load(f)\n",
    "    return data, path.split('/')[-1].split('.')[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%# A function that read a path of *.plist file and return a list of dictionaries and the name of the file without the extension\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def parse_files_name(string: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Read a file name and return a dict with key as student_id, passage_id and random_number\n",
    "    :param string: file name with schema student_{student_id}_passage_{passage_id}_{random_number}\n",
    "    :return: dict with key as student_id, passage_id and random_number\n",
    "    \"\"\"\n",
    "    student_id = string.split('_')[1]\n",
    "    passage_id = int(string.split('_')[3]) % 100000\n",
    "    random_number = string.split('_')[4]\n",
    "    return {'student_id': student_id, 'passage_id': passage_id, 'random_number': random_number}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def serialize_example(recordName, audioSegment, sample_rate, sentence, wordStart, wordDuration, matchSegment, matchReference):\n",
    "    \"\"\"\n",
    "    Creates a tf.train.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "    # data type.\n",
    "    feature = {\n",
    "        'RecordName': _bytes_feature(tf.io.serialize_tensor(recordName)),\n",
    "        'AudioSegment': _bytes_feature(tf.io.serialize_tensor(audioSegment)),\n",
    "        'SampleRate': _int64_feature(sample_rate),\n",
    "        'Sentence': _bytes_feature(tf.io.serialize_tensor(sentence)),\n",
    "        'WordStart': _bytes_feature(tf.io.serialize_tensor(wordStart)),\n",
    "        'WordDuration': _bytes_feature(tf.io.serialize_tensor(wordDuration)),\n",
    "        'MatchSegment': _bytes_feature(tf.io.serialize_tensor(matchSegment)),\n",
    "        'MatchReference': _bytes_feature(tf.io.serialize_tensor(matchReference)),\n",
    "    }\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 13:23:45.363812: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-12-03 13:23:45.363926: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Read file using dill\n",
    "with open('./unique_word.dill', 'rb') as f:\n",
    "    data = dill.load(f)\n",
    "data.add('')\n",
    "# data is a set convert it to numpy stored str array\n",
    "data_dict = tf.convert_to_tensor(sorted(list(data)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "p = Path('../Result')\n",
    "p2 = Path('../SiriR/SiriV1')\n",
    "available_Count = [1,2,4,5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "students = set(map(lambda item: int(parse_files_name(str(item.stem))['student_id']), list(p.glob('*.plist'))))\n",
    "students = sorted(list(students))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student id that split the train and test set is:  887\n"
     ]
    }
   ],
   "source": [
    "split_point = students[round(len(students) * 0.8)]\n",
    "print(\"The student id that split the train and test set is: \", split_point)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "533cf2e9166848f2a0713e68b5b16feb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student_921_passage_241174_56e850df4deb3\n",
      "student_137_passage_44000_553fb722737c1\n",
      "student_373_passage_233033_56e1b4b6872cf\n",
      "student_571_passage_22006_553aa46db5128\n",
      "student_19_passage_21002_5539d7e23cfdc\n",
      "student_818_passage_32023_553fb50ad4eee\n",
      "student_735_passage_241054_56df08bc62d3d\n",
      "student_818_passage_32023_553fb5ba447c9\n",
      "student_824_passage_32017_5548f520b2227\n",
      "student_587_passage_21075_553a6754d2c8f\n",
      "student_203_passage_221159_56e6f1d3a9529\n",
      "student_113_passage_222084_56e6fd8194d91\n",
      "student_123_passage_221146_56e6e6c3d8961\n",
      "student_309_passage_222077_56e6eb5908028\n",
      "student_605_passage_232062_56ddc99fed6e4\n",
      "student_373_passage_41162_553e7ede2ebcf\n",
      "student_473_passage_21010_553aa022ad0f1\n",
      "student_641_passage_231119_56e1cd00b2c3a\n",
      "student_309_passage_222084_56e6ea079ff21\n",
      "student_13_passage_221140_56e702b977999\n",
      "student_988_passage_31032_553fee9fb25c4\n",
      "student_575_passage_21051_553a6af7668b2\n",
      "student_581_passage_21037_553aa14e53972\n",
      "student_669_passage_231110_56ddb867b8a7e\n",
      "student_133_passage_222089_56df3f7fad83f\n",
      "student_567_passage_22031_5542961fa655c\n",
      "student_185_passage_222060_56df0e00b1f86\n",
      "student_698_passage_31061_553fbe70e0645\n",
      "student_818_passage_31061_553fb543032d8\n",
      "student_23_passage_31022_553a6191389f8\n",
      "student_107_passage_44000_553e6ac41a9c2\n",
      "student_447_passage_231091_56ddb4f8e96ba\n",
      "student_33_passage_221114_56df1d041f5f3\n",
      "student_655_passage_231078_56e1a675234c7\n",
      "student_507_passage_22013_553a852e30f0c\n",
      "student_587_passage_21002_553a66229a731\n",
      "student_583_passage_22040_553a6d0d37a4d\n",
      "student_722_passage_33013_5547ac2b6ff99\n",
      "student_722_passage_31041_5547ae05168dd\n",
      "student_15_passage_221125_56e845b05c1dd\n",
      "student_381_passage_21014_554288cdaa390\n",
      "student_273_passage_221126_56e6f196c8972\n",
      "student_955_passage_241104_56e9a630c054b\n",
      "student_818_passage_31057_553fb539bfd47\n",
      "student_567_passage_23025_554296e0c2643\n",
      "student_93_passage_223041_56e6f21cb8a2b\n",
      "student_1047_passage_41143_5564dc1329b17\n",
      "student_840_passage_31008_5548f37776d46\n",
      "student_267_passage_223021_56e6e76fa93c4\n",
      "student_1039_passage_33003_558c4409cd5d2\n",
      "student_117_passage_223039_56e8430bd6479\n",
      "student_119_passage_42029_553e69a4addbc\n",
      "student_641_passage_21088_554271158b140\n",
      "student_952_passage_31044_55522dad1d8ce\n",
      "student_515_passage_22007_553a7dfae1552\n",
      "student_691_passage_242063_56df07e3c027c\n",
      "student_295_passage_222071_56e6e2569d7f2\n",
      "student_37_passage_41095_553a62234acdf\n",
      "student_93_passage_221109_56e6f07397c9f\n",
      "student_39_passage_41012_5549349f2256c\n",
      "student_275_passage_223039_56e6e0f27a491\n",
      "student_241_passage_41157_554900fd41266\n",
      "student_371_passage_231102_56e1edc3bfede\n",
      "student_669_passage_231086_56ddb87e40358\n",
      "student_123_passage_222058_56e6e6937164c\n",
      "student_263_passage_221148_56df170c8cc81\n",
      "student_804_passage_31058_553fae382cf8b\n",
      "student_686_passage_31069_553fc77bf0a7d\n",
      "student_708_passage_31022_553fc730a461d\n",
      "student_664_passage_31034_553fc1839fef7\n",
      "student_818_passage_31044_553fb550981c3\n",
      "student_806_passage_31026_553fa9cfdc9bd\n",
      "student_601_passage_21037_553aa0299ccf3\n",
      "student_451_passage_22013_553a9a80ce6cd\n",
      "student_443_passage_21018_553a94c07db46\n",
      "student_115_passage_41028_553e6e4a5aff2\n",
      "student_45_passage_221164_56e05ea20a2ee\n",
      "student_1043_passage_22044_55957a694e68a\n",
      "student_37_passage_41143_553a623885067\n",
      "student_670_passage_32026_553fc67b05b0b\n",
      "student_1055_passage_222071_56dda98f13102\n",
      "student_804_passage_31067_553faeb0139a6\n",
      "student_651_passage_231141_56e069b408a14\n",
      "student_93_passage_222060_56e6f18302c5f\n",
      "student_295_passage_222089_56e6e1f1ba8ff\n",
      "student_1019_passage_24000_553a4b902294f\n",
      "student_267_passage_223019_56e6e7bab5a06\n",
      "student_859_passage_241104_56e8547547f61\n",
      "student_613_passage_231085_56ddc861a75fa\n",
      "student_669_passage_34000_56ddb6aab0f2e\n",
      "student_517_passage_231135_56ddb8d02af7e\n",
      "student_23_passage_31023_553a6197c5a4e\n",
      "student_507_passage_24000_553a80d708e8e\n",
      "student_515_passage_23042_553a7eff612f2\n",
      "student_331_passage_41081_553e7d9bc5c52\n",
      "student_185_passage_41055_553e5e56edb77\n",
      "student_309_passage_223038_56e6e9c95788c\n",
      "student_295_passage_24000_56e6e0873054f\n",
      "student_323_passage_41094_553e7c07bb218\n",
      "student_641_passage_21024_554270810314b\n",
      "student_17_passage_21011_553a617f60ef8\n",
      "student_211_passage_41144_5547be8a59b32\n",
      "student_708_passage_31019_553fc73e18824\n",
      "student_243_passage_221004_56e0589c1f31a\n",
      "student_579_passage_231085_56ddcbc130cdc\n",
      "student_167_passage_222089_56df482c98e75\n",
      "student_489_passage_233018_56ddb24b5bee7\n",
      "student_882_passage_34000_55523e57f34bc\n",
      "student_1021_passage_21043_553a621f80b04\n",
      "student_37_passage_41153_553a61a3cf89a\n",
      "student_678_passage_31019_553fc08bce0f9\n",
      "student_117_passage_221105_56e843a857efb\n",
      "student_123_passage_223043_56e6e5fdb7f78\n",
      "student_722_passage_31033_5547afb7ef6bf\n",
      "student_139_passage_41157_553e677d686ae\n",
      "student_385_passage_21088_5542860737d37\n",
      "student_1039_passage_33022_555e493a82844\n",
      "student_1041_passage_241170_56df0283009d2\n",
      "student_457_passage_231093_56ddd77d3f67d\n",
      "student_45_passage_221090_56e05df359f6f\n",
      "student_581_passage_21037_553aa1256757a\n",
      "student_5_passage_221148_56e6fc1c25c8e\n",
      "student_251_passage_221123_56df3f1823284\n",
      "student_818_passage_31063_553fb53de6980\n",
      "student_818_passage_32004_553fb5c79459f\n",
      "student_431_passage_34000_56ddd4461fd5d\n",
      "student_599_passage_21027_553aa44b47fa9\n",
      "student_587_passage_21079_553a678d44f87\n",
      "student_843_passage_241158_56df01fa78794\n",
      "student_309_passage_221105_56e6eaee40eb7\n",
      "student_970_passage_31034_553fe7726fbb1\n",
      "student_641_passage_231098_56e1cce841269\n",
      "student_339_passage_41064_553e809061f33\n",
      "student_457_passage_231141_56ddd705d0ab1\n",
      "student_674_passage_31048_553fcb863230c\n",
      "student_45_passage_221159_56e05c0df1e07\n",
      "student_233_passage_42020_554906a69b65a\n",
      "student_443_passage_231124_56ddae7bab0e0\n",
      "student_818_passage_31032_553fb52f1d6f3\n",
      "student_963_passage_241121_56e839d20a630\n",
      "student_883_passage_241166_56e9a68d953cf\n",
      "student_13_passage_222069_56e7058feccae\n",
      "student_251_passage_221059_56df3f30496e0\n",
      "student_579_passage_21007_553a70b2ec0e4\n",
      "student_309_passage_223039_56e6e9a47ce0e\n",
      "student_309_passage_221133_56e6ebcd31c63\n",
      "student_309_passage_221126_56e6ea69788b9\n",
      "student_579_passage_233040_56ddcb1d535e0\n",
      "student_167_passage_222071_56df488a4f996\n",
      "student_423_passage_231074_56ddd2233f6d4\n",
      "student_605_passage_24000_553a68fd40666\n",
      "student_641_passage_21021_554271f50b074\n",
      "student_383_passage_21010_55428e2b14a23\n",
      "student_804_passage_31026_553fae1d4eb33\n",
      "student_1049_passage_31041_559bfc3f1f22e\n",
      "student_295_passage_221006_56e6e2aac7fa4\n",
      "student_836_passage_31048_5548f44988cb8\n",
      "student_1055_passage_221148_56dda7a19fc6d\n",
      "student_593_passage_231102_56e1ac6f8c3fe\n",
      "student_93_passage_221137_56e6f09c13c6a\n",
      "student_457_passage_231131_56ddd7602f7ac\n",
      "student_575_passage_24000_553a694e92726\n",
      "student_19_passage_21003_5539d7fc073ef\n",
      "student_371_passage_233028_56e1ef24074bc\n",
      "student_722_passage_32002_5547b12690dd9\n",
      "student_295_passage_221103_56e6e28624ffa\n",
      "student_17_passage_24000_553a61ad4ed46\n",
      "student_473_passage_233035_56ddf1ae50267\n",
      "student_295_passage_221148_56e6e1ad31d18\n",
      "student_119_passage_222084_56e6fe93883f5\n",
      "student_722_passage_31053_5547af2f77403\n",
      "student_93_passage_221123_56e6f0bfba8fb\n",
      "student_409_passage_22036_55428db87c6ae\n",
      "student_818_passage_33024_553fb5d364f9e\n",
      "student_982_passage_31052_553fe9a767129\n",
      "student_309_passage_222048_56e6eb86e847c\n",
      "student_359_passage_41129_553e77f91a6b4\n",
      "student_135_passage_221103_56df222d915dd\n",
      "student_423_passage_231141_56ddd2de24506\n",
      "student_423_passage_231093_56ddd273da4d4\n",
      "student_325_passage_43046_553e79ae5fd51\n",
      "student_443_passage_231154_56ddad54eafc7\n",
      "student_79_passage_221125_56e6fc0bb3f2d\n",
      "student_487_passage_22040_553aa5e871968\n",
      "student_97_passage_43046_553e6caf25e3a\n",
      "student_373_passage_233028_56e1b49cae610\n",
      "student_700_passage_31008_553fc72dc2f22\n",
      "student_295_passage_221150_56e6e32f2134d\n",
      "student_507_passage_22043_553a8426486f9\n",
      "student_515_passage_21030_553a7c9698976\n",
      "student_702_passage_33014_553fc4a664801\n",
      "bytes length not a multiple of item size\n",
      "student_251_passage_221137_56df3f41d7bb8\n",
      "student_309_passage_221115_56e6eaadb84db\n",
      "student_722_passage_31025_5547aeef5cb49\n",
      "student_669_passage_231130_56ddb9005620a\n",
      "student_295_passage_222059_56e6e2f8b635b\n",
      "student_115_passage_44000_553e6bcd03835\n",
      "student_493_passage_21010_553a7fb2036b9\n",
      "student_595_passage_231102_56ddb176388c9\n",
      "bytes length not a multiple of item size\n",
      "student_535_passage_23025_554299999be58\n",
      "student_722_passage_33020_5547abe69b4ff\n",
      "student_818_passage_31058_553fb54ce69ea\n"
     ]
    }
   ],
   "source": [
    "# Generate a Tensorflow Record File\n",
    "with tf.io.TFRecordWriter('./Student_Answer_Record_Train.tfrecord', options=tf.io.TFRecordOptions(compression_type='GZIP')) as writerTrain:\n",
    "    with tf.io.TFRecordWriter('./Student_Answer_Record_Eval.tfrecord', options=tf.io.TFRecordOptions(compression_type='GZIP')) as writerEval:\n",
    "        for file in tqdm(p.glob('*.plist')):\n",
    "            record, name = read_plist(str(file))\n",
    "            #Read student wave file by name path under ../Audio\n",
    "            try:\n",
    "                audioHandle = AudioSegment.from_file('../Audio/{}.wav'.format(name), format='wav')\n",
    "                #Save audio files as numpy convert to float and normalize it using np.int16\n",
    "                # audioSegment = np.array(audioHandle.get_array_of_samples()).astype(np.float32) / np.iinfo(np.int16).max\n",
    "                audioRawArray = audioHandle.get_array_of_samples()\n",
    "                audioSegment = tf.constant(audioRawArray, dtype=tf.int16)\n",
    "                sample_rate = audioHandle.frame_rate\n",
    "            except Exception as e:\n",
    "                print(name)\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            name_info = parse_files_name(name)\n",
    "            try:\n",
    "                matchSegments = []\n",
    "                matchReferences = []\n",
    "                for ii in available_Count:\n",
    "                    record2, name2 = read_plist(str(p2.parent / 'SiriV{}'.format(ii) / '{}.plist'.format(name_info['passage_id'])))\n",
    "                    try:\n",
    "                        T, pRecord2, pRecord=lcss(record2, record, stemmer=None)\n",
    "                    except:\n",
    "                        print(name)\n",
    "                        raise Exception\n",
    "                    pairs = list(zip(pRecord2, pRecord))\n",
    "                    numpy_pairs = np.array(pairs).astype(np.int64)\n",
    "                    try:\n",
    "                        matchSegments.append(numpy_pairs[:, 1])\n",
    "                        matchReferences.append(numpy_pairs[:, 0])\n",
    "                    except IndexError:\n",
    "                        assert len(pairs) == 0\n",
    "                        raise IndexError\n",
    "            except IndexError:\n",
    "                assert len(pairs) == 0\n",
    "                print(name)\n",
    "                continue\n",
    "            #pad matchSegments to same length using keras pad_sequence\n",
    "            matchSegment = tf.keras.preprocessing.sequence.pad_sequences(matchSegments, padding='post',value=-1, dtype=np.int64) + 1\n",
    "            matchReference = tf.keras.preprocessing.sequence.pad_sequences(matchReferences, padding='post',value=-1, dtype=np.int64) + 1\n",
    "\n",
    "            recordName = name\n",
    "            dataframe = pd.DataFrame(record)\n",
    "            wordStart = dataframe['tTime'].values.astype(np.float32)\n",
    "            #pad a -1 at front\n",
    "            wordStart = np.insert(wordStart, 0, -1)\n",
    "            wordDuration = dataframe['tDuration'].values.astype(np.float32)\n",
    "            #pad a -1 at front\n",
    "            wordDuration = np.insert(wordDuration, 0, -1)\n",
    "            stemmer = nltk.stem.LancasterStemmer()\n",
    "            stemmed_map = map(lambda x: stemmer.stem(x), dataframe['tString'])\n",
    "            #Generate Sparse index using location of data_dict\n",
    "            sparse_index = map(lambda x: x == data_dict, stemmed_map)\n",
    "            #Convert to dense index from sparse index using argMax\n",
    "            dense_index = map(lambda x: np.argmax(x), sparse_index)\n",
    "            #convert to int tensor\n",
    "            dense_index = tf.constant(list(dense_index), dtype=tf.int64)\n",
    "            #pad a zero at front\n",
    "            sentence = tf.pad(dense_index, [[1, 0]], constant_values=-1)\n",
    "            #Serialize the example\n",
    "            example = serialize_example(recordName, audioSegment, sample_rate, sentence, wordStart, wordDuration, matchSegment, matchReference)\n",
    "            if int(name_info['student_id']) <= split_point:\n",
    "                writerTrain.write(example)\n",
    "            else:\n",
    "                writerEval.write(example)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d2e22bf35174595896af8910826204a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process Reference under Siri Folder Audio and save it as a single serialized files\n",
    "p = Path('../Siri/')\n",
    "available_Count = [1,2,4,5]\n",
    "#Get all m4a files under SiriR/SiriV1 and return passage id\n",
    "available_Sample = map(lambda x: int(x.stem) % 100000, p.glob('SiriV1/*.m4a'))\n",
    "#Use a loop to get all Audios using passage id and available count, different sample storage in different folder with name SiriV1, SiriV2, SiriV3, SiriV4, SiriV5, the available count defined in available_Count\n",
    "for sample in tqdm(available_Sample):\n",
    "    audio_names = map(lambda x: str(p / 'SiriV{}/{}.m4a'.format(x, sample)), available_Count)\n",
    "    audio_handles = map(lambda x: AudioSegment.from_file(x, format='m4a'), audio_names)\n",
    "    #get audio segment in original format\n",
    "    audio_segments = map(lambda x: np.array(x.get_array_of_samples()), audio_handles)\n",
    "    #paddding audio segment to the same length and stack it in to a tensor\n",
    "    audio_segments = tf.constant(tf.keras.preprocessing.sequence.pad_sequences(list(audio_segments), padding='post', value=0, dtype=np.int16))\n",
    "    audio_segment = tf.stack(list(audio_segments))\n",
    "    #save the tensor to disk under Siri_Reference_Sample folder create the folder if not exist\n",
    "    tf.io.write_file('./Siri_Reference_Sample/{}.tfs'.format(sample), tf.io.serialize_tensor(audio_segment))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#create the parser function to parse the serialized generated above\n",
    "def parse_function(serialized_example):\n",
    "    # Define a dict with the data-names and types we expect to find in the\n",
    "    # serialized example.\n",
    "    features = {\n",
    "        'RecordName': tf.io.FixedLenFeature([], tf.string),\n",
    "        'AudioSegment': tf.io.FixedLenFeature([], tf.string),\n",
    "        'SampleRate': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'Sentence': tf.io.FixedLenFeature([], tf.string),\n",
    "        'WordStart': tf.io.FixedLenFeature([], tf.string),\n",
    "        'WordDuration': tf.io.FixedLenFeature([], tf.string),\n",
    "        'MatchSegment': tf.io.FixedLenFeature([], tf.string),\n",
    "        'MatchReference': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    e = tf.io.parse_single_example(serialized_example, features)\n",
    "    #Convert the serialized tensor to tensor\n",
    "    e['AudioSegment'] = tf.io.parse_tensor(e['AudioSegment'], out_type=tf.int16)\n",
    "    e['Sentence'] = tf.io.parse_tensor(e['Sentence'], out_type=tf.string)\n",
    "    e['WordStart'] = tf.io.parse_tensor(e['WordStart'], out_type=tf.float32)\n",
    "    e['WordDuration'] = tf.io.parse_tensor(e['WordDuration'], out_type=tf.float32)\n",
    "    e['MatchSegment'] = tf.io.parse_tensor(e['MatchSegment'], out_type=tf.int64)\n",
    "    e['MatchReference'] = tf.io.parse_tensor(e['MatchReference'], out_type=tf.int64)\n",
    "    return e"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#Create the dataset by tfrecord file generated above\n",
    "dataset = tf.data.TFRecordDataset('./Student_Answer_Record.tfrecord', compression_type='GZIP')\n",
    "dataset = dataset.map(parse_function)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# processed_passage = set()\n",
    "# for item in dataset:\n",
    "#     passage_id = parse_files_name(item['RecordName'].numpy().decode('utf-8'))['passage_id']\n",
    "#     # if passage id exist skip to next item\n",
    "#     if passage_id in processed_passage:\n",
    "#         continue\n",
    "#     processed_passage.add(passage_id)\n",
    "#     sent = list(item['Sentence'].numpy())\n",
    "#     decoded_map = map(lambda x: x.decode('utf-8'), sent)\n",
    "#     #Stem data using Lancaster Stemmer\n",
    "#     stemmer = nltk.stem.LancasterStemmer()\n",
    "#     stemmed_map = map(lambda x: stemmer.stem(x), decoded_map)\n",
    "#     #Generate Sparse index using location of data_dict\n",
    "#     sparse_index = map(lambda x: x == data_dict, stemmed_map)\n",
    "#     #Convert to dense index from sparse index using argMax\n",
    "#     dense_index = map(lambda x: np.argmax(x), sparse_index)\n",
    "#     #convert to int tensor\n",
    "#     dense_index = tf.constant(list(dense_index), dtype=tf.int64)\n",
    "#     #save dense index to disk using passage id as file name\n",
    "#     tf.io.write_file('./Siri_Dense_Index/{}.tfs'.format(passage_id), tf.io.serialize_tensor(dense_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b73af559a88f41bf957ca76e1f8a96e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iterate over path p2 and save stem sentence to disk\n",
    "for passage_id in tqdm(map(lambda x: int(x.stem) ,p2.glob('*.plist'))):\n",
    "    times = []\n",
    "    words = []\n",
    "    for i in available_Count:\n",
    "        path = p2.parent / 'SiriV{}'.format(i) / '{}.plist'.format(passage_id)\n",
    "        #read and parse plist file\n",
    "        record, name = read_plist(str(path))\n",
    "        #convert dict record to pd dDataFrame\n",
    "        dataframe = pd.DataFrame(record)\n",
    "        #get passage id from name\n",
    "        #Stem data using Lancaster Stemmer\n",
    "        stemmer = nltk.stem.LancasterStemmer()\n",
    "        stemmed_map = map(lambda x: stemmer.stem(x), dataframe['tString'])\n",
    "        #Generate Sparse index using location of data_dict\n",
    "        sparse_index = map(lambda x: x == data_dict, stemmed_map)\n",
    "        #Convert to dense index from sparse index using argMax\n",
    "        dense_index = map(lambda x: np.argmax(x), sparse_index)\n",
    "        #convert to int tensor\n",
    "        dense_index = tf.constant(list(dense_index), dtype=tf.int64)\n",
    "        #Reference stack starting point and period\n",
    "        start = dataframe['tTime'].to_numpy().astype(np.float32)\n",
    "        period = dataframe['tDuration'].to_numpy().astype(np.float32)\n",
    "        total = tf.stack([start, period], axis=-1)\n",
    "        times.append(total)\n",
    "        words.append(dense_index)\n",
    "    #stack all the reference\n",
    "    #paddding audio segment to the same length and stack it in to a tensor\n",
    "    times = tf.constant(tf.keras.preprocessing.sequence.pad_sequences(times, padding='post',value=-1, dtype=np.float32))\n",
    "    times = tf.stack(times, axis=0)\n",
    "    #pad a -1 to begin of second axis to indicate the start of the slot times has shape (4, None, 2)\n",
    "    times = tf.pad(times, [[0,0],[1,0],[0,0]], constant_values=-1)\n",
    "    words = tf.constant(tf.keras.preprocessing.sequence.pad_sequences(words, padding='post',value=-1, dtype=np.int64))\n",
    "    words = tf.stack(words, axis=0)\n",
    "    #pad a -1 to words to indicate the start of the slot word has shape (4, None) pad to second axis\n",
    "    words = tf.pad(words, [[0,0],[1,0]], constant_values=-1)\n",
    "    #save total to disk using passage id as file name\n",
    "    tf.io.write_file('./Siri_Dense_Index/{}_ref.tfs'.format(passage_id), tf.io.serialize_tensor(times))\n",
    "    #save dense index to disk using passage id as file name\n",
    "    tf.io.write_file('./Siri_Dense_Index/{}_word.tfs'.format(passage_id), tf.io.serialize_tensor(words))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "te = tf.io.parse_tensor(tf.io.read_file('./Siri_Dense_Index/{}_ref.tfs'.format(41166)), out_type=tf.float32)\n",
    "tw = tf.io.parse_tensor(tf.io.read_file('./Siri_Dense_Index/{}_word.tfs'.format(41166)), out_type=tf.int64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m = tf.convert_to_tensor(matchReference)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.gather(tw, m, batch_dims=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}