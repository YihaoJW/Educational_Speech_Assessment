import tensorflow as tf
import keras_nlp


class RotaryEmbeddingMask(keras_nlp.layers.RotaryEmbedding):
    def compute_mask(self, inputs, mask=None):
        return mask


class BaseAttention(tf.keras.layers.Layer):
    """
    A basic multi-head attention layer
    """

    def __init__(self, **kwargs):
        super().__init__()
        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)
        self.layer_norm = tf.keras.layers.LayerNormalization()
        self.add = tf.keras.layers.Add()

        def calculate_attention(x, y, mask):
            # if tf.get_static_value(tf.rank(mask)) == 0:
            #     mask = None

            attn_output = self.mha(
                query=x,
                value=y,
                key=y,
                attention_mask=mask
            )
            result = self.add([x, attn_output])
            return result

        self.calculate_attention = tf.recompute_grad(calculate_attention)

    def get_config(self):
        config = super().get_config()
        config.update({"mha": self.mha.get_config(),
                       "layer_norm": self.layer_norm.get_config(),
                       "add": self.add.get_config()})
        return config

    @staticmethod
    def generate_attention_mask(mask_x, mask_y):
        """Generate the cross-attention mask.

        Args:
            mask_x: Mask for the query, could be None.
            mask_y: Mask for the key, could be None.

        Returns:
            A Tensor representing the cross attention mask, or None if both mask_x and mask_y are None.
        """

        # If mask_y (key mask) is None, return all True attention mask
        if mask_y is None:
            return tf.ones([1, 1, 1])

        # Only expand dimensions of mask_y for keys, as we only want to mask the keys
        return tf.expand_dims(mask_y, 1)

    def compute_mask(self, inputs, mask=None):
        # Just pass the received mask from the previous layer to the next layer or
        # manipulate it if this layer changes the shape of the input
        return mask


class SelfAttention(BaseAttention):
    """
    A self-attention layer, that using gradient check point to save memory
    """

    @tf.function
    def call(self, inputs, training=None, mask=None):
        x = inputs
        # calculate self-attention mask, mask is a function parameter generated by keras Mask has shape (batch_size,
        # seq_len) with boolean values, True means the token is masked, False means the token is not masked

        attention_mask = self.generate_attention_mask(mask, mask)
        x = self.calculate_attention(x, x, attention_mask)
        x = self.layer_norm(x)
        # mask of X is the same as the input mask
        x._keras_mask = mask
        return x


class CrossAttention(BaseAttention):

    @tf.function
    def call(self, inputs, training=None, mask=None):
        x, y = inputs
        # Calculate cross-attention mask, mask is a function parameter generated by keras Mask has shape (batch_size,
        # seq_len) with boolean values, True means the token is masked, False means the token is not masked
        # Y is the main value, X is a query
        mask_x, mask_y = mask if mask is not None else (None, None)
        attention_mask = self.generate_attention_mask(mask_x, mask_y)

        y = self.calculate_attention(x, y, attention_mask)
        y = self.layer_norm(y)
        # mask of Y is the same as the input mask
        y._keras_mask = mask_y
        return y


class PositionEncoding1D(tf.keras.layers.Layer):
    def __init__(self, default_position=128, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.built = None
        self.pos_encoding = None
        self.d_model = None
        self.def_position = default_position
        with tf.init_scope():
            self.position = self.def_position
        self.mask = tf.keras.layers.Masking(mask_value=-1.0)

    def compute_mask(self, inputs, mask=None):
        return self.mask.compute_mask(inputs, mask)

    def get_config(self):
        config = super().get_config()
        config.update({"def_position": self.def_position})
        return config

    def build(self, input_shape):
        assert input_shape[-1] is not None, "Unit cannot undefined"
        self.d_model = input_shape[-1]
        if input_shape[-2] is None:
            self.position = self.def_position
        else:
            self.position = input_shape[-2]
        self.pos_encoding = self.positional_encoding()
        self.built = True

    def positional_encoding(self):
        angle_rates = 1 / tf.pow(10000.,
                                 tf.cast(tf.range(0, self.d_model, 2), self.dtype_policy.variable_dtype) / tf.cast(
                                     self.d_model, self.dtype_policy.variable_dtype))
        angle_rads = tf.einsum('i,j->ij', tf.cast(tf.range(self.position), self.dtype_policy.variable_dtype),
                               angle_rates)
        sin_cos = tf.math.sin(angle_rads)[..., tf.newaxis], tf.math.cos(angle_rads)[..., tf.newaxis]
        pos_encoding = tf.reshape(tf.concat(sin_cos, axis=-1), [angle_rads.shape[0], -1])[:, :self.d_model]
        return pos_encoding

    def get_encode(self, length, x, tig=5):
        encoder_msg = tf.cast(self.pos_encoding[0: length][tf.newaxis, ...], self.dtype_policy.compute_dtype)
        try:
            encode = encoder_msg + x
        except tf.errors.InvalidArgumentError:
            self.position = length if length > self.position * 2 else self.position * 2
            self.pos_encoding = self.positional_encoding()
            assert tig > 0, "Too much iteration, might caused by feature map size change"
            encode = self.get_encode(length, x, tig=tig - 1)
        return encode

    def call(self, x, training=False, mask=None):
        x_length = tf.shape(x)[-2]
        return self.get_encode(x_length, x)


# Define a Concatenate layer that will cut the input to the same length
class CutConcatenate(tf.keras.layers.Concatenate):

    def compute_mask(self, inputs, mask=None):
        if mask is None:
            return None

        masks = []
        for mask_i in mask:
            if mask_i is not None:
                # If input mask is not None, cut the mask to match the first input's dimensions
                sliced_mask = tf.slice(mask_i, [0, 0], [-1, tf.shape(mask[0])[1]])
                masks.append(sliced_mask)
            else:
                masks.append(None)

        # Call super compute_mask which handles concatenation of masks and
        # returns a mask that corresponds to the minimum presence of 1s across all masks
        return super().compute_mask(masks)

    def call(self, inputs):
        inter = [tf.slice(x, [0, 0, 0], [-1, tf.shape(inputs[0])[1], -1]) for x in inputs]
        # Set shape of interring to the shape of inputs if input is Tensor Placeholder
        shapes = [x.shape for x in inputs]
        for y, x in zip(inter, inputs):
            d = [sx if sx is not None and sy is None else sy for sx, sy in zip(x.shape, y.shape)]
            y.set_shape(d)

        return super().call(inter)
